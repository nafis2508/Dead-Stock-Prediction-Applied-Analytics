{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Analytics BUSA8001- Applied Predictive Analytics\n",
    "## S2 2025\n",
    "##  *Programming Task 1* {-}\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assignment Points**: 100  (*worth 30% of total Grade*)\n",
    "\n",
    "**Due Date**: Friday Week 8 (19 September 2025) @ 11.59pm  \n",
    "**Submission**: Provide your answers in this Jupypter notebook and submit it via iLearn link  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Predictive Analysis of Inventory Dead stock**\n",
    "\n",
    "### Objective: \n",
    "- This assignment focuses on a dataset of Inventory dead stock.\n",
    "- The primary goal is to predict which stock items are likely to be dead (e.g. not used or moved in a long time) using various data mining methods.\n",
    "\n",
    "### Background: \n",
    "A mid manufacturing retailer noticed that certain items in its warehouse across Australia had not moved for a number of months. These products included a number of products or materials stored or manufactured in the wharehouses. Despite being listed in the inventory system, they had no recent sales activity, were no longer featured in marketing campaigns, and had limited relevance to current customer needs.\n",
    "\n",
    "The inventory manager initiated a review to determine whether these items should be classified as dead stock—inventory that is unlikely to be sold due to obsolescence, lack of demand, damage, or other reasons that can impact their classification. The classification would allow the company to make informed decisions about markdowns, liquidation, or disposal, and free up valuable warehouse space. This project aims to refine this classification by identifying stock items (and possibly their wharehouses locations) that are likely to be dead stock, enhancing the precision of wharhouse facility storage planning\n",
    "\n",
    "Target variable\n",
    "- Dead stock (1=yes, 0=no)\n",
    "\n",
    "The dataset contains the features listed in the second **\"Readme\"** sheet of the Dataset file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Problem 1** - Reading the dataset (Total Marks: 20)\n",
    "\n",
    "\n",
    "\n",
    "**Q1**. Create a pandas dataframe contining the first 3,000 rows from the Inventory dataset provided in the **Dataset** folder \n",
    "- Delete 'Item No.' column\n",
    "- Print `info()` of the dataframe\n",
    "\n",
    "(2 marks)\n",
    "\n",
    "- Do you recommend dropping/deleting any another column(s) from the Inventory dataset? \n",
    "\n",
    "If **YES** then:\n",
    "1- Identify which one(s) and\n",
    "2- Justify your selection in one sentence\n",
    "\n",
    "If **NO** then Justify your answer\n",
    "\n",
    "(3 marks) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 31 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   Whse                              3000 non-null   object \n",
      " 1   State                             3000 non-null   object \n",
      " 2   Base Unit                         3000 non-null   object \n",
      " 3   Total - Quantity                  3000 non-null   float64\n",
      " 4   Inventory Aging Report Unit Cost  3000 non-null   float64\n",
      " 5   Total - Value                     3000 non-null   float64\n",
      " 6   6 Months QTY                      3000 non-null   float64\n",
      " 7   12 Months QTY                     3000 non-null   float64\n",
      " 8   2 Years QTY                       3000 non-null   float64\n",
      " 9   Over 2 Years Qty                  3000 non-null   float64\n",
      " 10  Over 3 Years Quantity             3000 non-null   float64\n",
      " 11  Business Area                     3000 non-null   object \n",
      " 12  Item Type                         3000 non-null   object \n",
      " 13  ABC Class                         3000 non-null   object \n",
      " 14  Jan                               3000 non-null   float64\n",
      " 15  Feb                               3000 non-null   float64\n",
      " 16  Mar                               3000 non-null   float64\n",
      " 17  Apr                               3000 non-null   float64\n",
      " 18  May                               3000 non-null   float64\n",
      " 19  Jun                               3000 non-null   float64\n",
      " 20  Jul                               3000 non-null   float64\n",
      " 21  Aug                               3000 non-null   float64\n",
      " 22  Sep                               3000 non-null   float64\n",
      " 23  Oct                               3000 non-null   float64\n",
      " 24  Nov                               3000 non-null   float64\n",
      " 25  Dec                               3000 non-null   float64\n",
      " 26  Avg monthly                       3000 non-null   float64\n",
      " 27  Inventory Turn                    3000 non-null   float64\n",
      " 28  Over 2 years Qty                  3000 non-null   float64\n",
      " 29  % of over 2 year                  3000 non-null   float64\n",
      " 30  Dead stock                        3000 non-null   object \n",
      "dtypes: float64(24), object(7)\n",
      "memory usage: 726.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Loading pandas and giving it the alias pd\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset and keeping only the first 3,000 rows\n",
    "df = pd.read_excel(\"~/Desktop/Programming_Task1_S2_2025/Dataset/Inventory.xlsx\").iloc[:3000].copy()\n",
    "\n",
    "# Dropping the identifier column and item description \n",
    "df.drop(columns=[\"Item No. \"], inplace=True)\n",
    "df.drop(columns=[\"Item Description\"], inplace=True)\n",
    "\n",
    "# Printing structure of the dataframe\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "Yes, I recommend also dropping 'Item Description' because it is a long free-text field that does not provide predictive value for modeling and only adds noise to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. List which **features** are *numeric*, *ordinal*, and *nominal* variables, and how many features of each kind there are in the dataset.\n",
    "To answer this question \n",
    "\n",
    "- Find the definitions of numeric, ordinal and nominal variables in the course material    \n",
    "- Carefully consider what values each feature can take as well as the output of `df.info()`.\n",
    "- Make sure to check the **Readme** sheet in the dataset to understand the features description  \n",
    "\n",
    "Your answer should be written up in Markdown and include:\n",
    "1) A table listing all the features present in the dataset and their type (fill out the table template provided below) and\n",
    "2) A brief description of the contents of the table.\n",
    "\n",
    "|Variable Kind|Number of Features|Feature Names\n",
    "| --- | --- | --- |\n",
    "| Numeric | some number | some text |\n",
    "| some text  | some number | some text |\n",
    "| some text  | some number | some text |\n",
    "\n",
    "\n",
    "(10 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 31 columns):\n",
      " #   Column                            Non-Null Count  Dtype  \n",
      "---  ------                            --------------  -----  \n",
      " 0   Whse                              3000 non-null   object \n",
      " 1   State                             3000 non-null   object \n",
      " 2   Base Unit                         3000 non-null   object \n",
      " 3   Total - Quantity                  3000 non-null   float64\n",
      " 4   Inventory Aging Report Unit Cost  3000 non-null   float64\n",
      " 5   Total - Value                     3000 non-null   float64\n",
      " 6   6 Months QTY                      3000 non-null   float64\n",
      " 7   12 Months QTY                     3000 non-null   float64\n",
      " 8   2 Years QTY                       3000 non-null   float64\n",
      " 9   Over 2 Years Qty                  3000 non-null   float64\n",
      " 10  Over 3 Years Quantity             3000 non-null   float64\n",
      " 11  Business Area                     3000 non-null   object \n",
      " 12  Item Type                         3000 non-null   object \n",
      " 13  ABC Class                         3000 non-null   object \n",
      " 14  Jan                               3000 non-null   float64\n",
      " 15  Feb                               3000 non-null   float64\n",
      " 16  Mar                               3000 non-null   float64\n",
      " 17  Apr                               3000 non-null   float64\n",
      " 18  May                               3000 non-null   float64\n",
      " 19  Jun                               3000 non-null   float64\n",
      " 20  Jul                               3000 non-null   float64\n",
      " 21  Aug                               3000 non-null   float64\n",
      " 22  Sep                               3000 non-null   float64\n",
      " 23  Oct                               3000 non-null   float64\n",
      " 24  Nov                               3000 non-null   float64\n",
      " 25  Dec                               3000 non-null   float64\n",
      " 26  Avg monthly                       3000 non-null   float64\n",
      " 27  Inventory Turn                    3000 non-null   float64\n",
      " 28  Over 2 years Qty                  3000 non-null   float64\n",
      " 29  % of over 2 year                  3000 non-null   float64\n",
      " 30  Dead stock                        3000 non-null   object \n",
      "dtypes: float64(24), object(7)\n",
      "memory usage: 726.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---- provide your text answer here ----\n",
    "| Variable Kind | Number of Features | Feature Names |\n",
    "|---------------|--------------------|---------------|\n",
    "| Numeric       | 24                 | Total - Quantity, Inventory Aging Report Unit Cost, Total - Value, 6 Months QTY, 12 Months QTY, 2 Years QTY, Over 2 Years Qty, Over 3 Years Quantity, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec, Avg monthly, Inventory Turn, Over 2 years Qty, % of over 2 year |\n",
    "| Nominal       | 6                  | ABC Class, Whse, State, Base Unit, Business Area, Item Type |\n",
    "\n",
    "The table shows that most features (24) are numeric, measuring stock quantities, costs, values, and turnover. The remaining features (6) are nominal, representing categorical labels such as warehouse, state, business area, item type, and ABC Class. No features are truly ordinal, as ABC Class mixes categories that are not strictly ranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3.** Missing Values. \n",
    "\n",
    "- Print out the number of missing values for each variable in the dataset and comment on your findings.\n",
    "\n",
    "(5 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whse                                0\n",
      "State                               0\n",
      "Base Unit                           0\n",
      "Total - Quantity                    0\n",
      "Inventory Aging Report Unit Cost    0\n",
      "Total - Value                       0\n",
      "6 Months QTY                        0\n",
      "12 Months QTY                       0\n",
      "2 Years QTY                         0\n",
      "Over 2 Years Qty                    0\n",
      "Over 3 Years Quantity               0\n",
      "Business Area                       0\n",
      "Item Type                           0\n",
      "ABC Class                           0\n",
      "Jan                                 0\n",
      "Feb                                 0\n",
      "Mar                                 0\n",
      "Apr                                 0\n",
      "May                                 0\n",
      "Jun                                 0\n",
      "Jul                                 0\n",
      "Aug                                 0\n",
      "Sep                                 0\n",
      "Oct                                 0\n",
      "Nov                                 0\n",
      "Dec                                 0\n",
      "Avg monthly                         0\n",
      "Inventory Turn                      0\n",
      "Over 2 years Qty                    0\n",
      "% of over 2 year                    0\n",
      "Dead stock                          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Checking number of missing values for each column\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "The results show that no variables in the dataset contain missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 2.** Cleaning data and dealing with categorical features (Total Marks: 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** \n",
    "\n",
    "- Use an appropriate `pandas` function to impute missing values using one of the following two strategies: `mean` and `mode`. (10 marks)\n",
    "    - Take into consideration the type of each variable (as in Q2 above) and the best practices we discussed in class/lecture notes\n",
    "- Explain what data imputation is, how you have done it here, and what decisions you had to make. (5 marks)\n",
    "\n",
    "\n",
    "(Total: 15 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "# Filling missing values in numeric columns with mean\n",
    "for col in df.select_dtypes(include=['float64', 'int64']).columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "# Filling missing values in categorical columns with mode\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "Data imputation is the process of filling in missing or invalid values so the dataset remains complete and suitable for analysis. In this case, I followed best practices by imputing numeric variables with their mean values, which preserves the overall distribution of continuous data without biasing towards extreme values. For categorical variables, I used the mode, as it ensures missing entries are replaced with the most frequent and representative category. This strategy keeps the dataset both consistent and meaningful, allowing the models to learn from patterns without being distorted by gaps in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "- Print `value_counts()` of the 'State' column and add a dummy variable named 'State_NSW' to `df` using `get_dummies()` (3 marks)\n",
    "- Carefully explain what the values of the new variable 'State_NSW' mean (2 mark)\n",
    "- Make sure the variable 'State' is deleted from the original dataframe   \n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State\n",
      "NSW    2113\n",
      "WA      887\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Checking the frequency distribution of the 'State' column\n",
    "print(df['State'].value_counts())\n",
    "\n",
    "# Creating dummy variables for 'State' using get_dummies\n",
    "state_dummies = pd.get_dummies(df['State'], prefix='State', dtype=int)\n",
    "\n",
    "# Keeping only the 'State_NSW' dummy\n",
    "df['State_NSW'] = state_dummies['State_NSW']\n",
    "\n",
    "# Dropping the original 'State' column\n",
    "df.drop(columns=['State'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "The frequency distribution shows that most observations belong to NSW, while a smaller proportion belong to WA. The new dummy variable State_NSW takes the value 1 if the state is NSW and 0 if the state is WA. This transformation encodes the categorical state information into a numerical format, making it suitable for predictive modeling, while ensuring no observations were lost when the original State column was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. Print `value_counts()` of the 'Whse' column and *carefully* comment on what you notice in relation to the definition of this variable. \n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whse\n",
      "1N1    1318\n",
      "1W0     887\n",
      "1N0     795\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "# Checking the frequency distribution of 'Whse' column\n",
    "print(df['Whse'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "The Whse column represents warehouse codes that identify different storage locations. The distribution shows that most items are stored in warehouse 1N1 (1318 records), followed by 1W0 (887 records) and 1N0 (795 records). This confirms that Whse is a categorical variable describing warehouse sites rather than a numeric measure, consistent with the dataset definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q4**. \n",
    "\n",
    "- Apply `get_dummies()` to 'Whse' feature and add dummy variables 'Whse_1N0', 'Whse_1N1', 'Whse_1W0' to `df`. (5 marks)   \n",
    "- *Carefully consider* how to allocate all the values of 'Whse' across these 3 newly created features (5 marks)\n",
    "    - Do not delete observations \n",
    "    - Do not assume that the anomaly are missing observations      \n",
    "    - Explain what decision you made\n",
    "- Make sure that 'Whse' is deleted from `df`   \n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "# Applying get_dummies() to 'Whse' column to create dummy variables\n",
    "whse_dummies = pd.get_dummies(df['Whse'], prefix='Whse', dtype=int, drop_first=True)\n",
    "\n",
    "df = df.join(whse_dummies)\n",
    "\n",
    "# Explicitly drop the original 'Whse' column (already handled by get_dummies, but done here for clarity)\n",
    "df.drop(columns=['Whse'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "When I used get_dummies() to the Whse column created dummy variables for each warehouse, but with drop_first=True one category was dropped to serve as the baseline. Since the dataset only contains warehouses 1N0, 1N1, and 1W0, the dummy variables now indicate membership relative to the dropped category. For example, if Whse_1N1 = 1, the observation belongs to warehouse 1N1; if all dummy variables equal 0, the observation belongs to the baseline warehouse. The original Whse column was deleted to avoid duplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q5**. In the column 'Item Type', convert the values of all items that are FG --- into \"Finished Goods\", RM --- into \"Raw Materials\", keep \"WIP Manufactured\", while combine all others like [Subcontract (9), Customer Supplied (08), and  zItems] into type 'Other' so you have Four Item types **Finished Goods**, **Raw Materials**, **WIP Manufactured**, and **Other**. \n",
    "\n",
    "Now Encode all the remaining non numeric features using appropriate encoding method presented in class\n",
    "\n",
    "(Total: 5 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df.loc[df['Item Type'].str.startswith(\"FG\"), 'Item Type'] = \"Finished Goods\"\n",
    "df.loc[df['Item Type'].str.startswith(\"RM\"), 'Item Type'] = \"Raw Materials\"\n",
    "df.loc[df['Item Type'] == \"WIP Manufactured (2)\", 'Item Type'] = \"WIP Manufactured\"\n",
    "df.loc[~df['Item Type'].isin([\"Finished Goods\", \"Raw Materials\", \"WIP Manufactured\"]), 'Item Type'] = \"Other\"\n",
    "\n",
    "\n",
    "item_dummies = pd.get_dummies(df['Item Type'], prefix='ItemType', dtype=int, drop_first=True)\n",
    "\n",
    "df = df.join(item_dummies)\n",
    "\n",
    "# Explicitly dropping the original 'Item Type' column (already handled by get_dummies, but done here for clarity)\n",
    "df.drop(columns=['Item Type'], inplace=True)\n",
    "\n",
    "\n",
    "baseunit_dummies = pd.get_dummies(df['Base Unit'], prefix='BaseUnit', dtype=int, drop_first=True)\n",
    "\n",
    "df = df.join(baseunit_dummies)\n",
    "\n",
    "# Explicitly dropping the original 'Base Unit' column (already handled by get_dummies, but done here for clarity)\n",
    "df.drop(columns=['Base Unit'], inplace=True)\n",
    "\n",
    "businessarea_dummies = pd.get_dummies(df['Business Area'], prefix='BusinessArea', dtype=int, drop_first=True)\n",
    "\n",
    "df = df.join(businessarea_dummies)\n",
    "\n",
    "# Explicitly dropping the original 'Business Area' column (already handled by get_dummies, but done here for clarity)\n",
    "df.drop(columns=['Business Area'], inplace=True)\n",
    "\n",
    "\n",
    "abc_dummies = pd.get_dummies(df['ABC Class'], prefix='ABCClass', dtype=int, drop_first=True)\n",
    "\n",
    "df = df.join(abc_dummies)\n",
    "\n",
    "# Explicitly drop the original 'Base Unit' column (already handled by get_dummies, but done here for clarity)\n",
    "df.drop(columns=['ABC Class'], inplace=True)\n",
    "\n",
    "\n",
    "# Label-encode target variable 'Dead stock'\n",
    "le = LabelEncoder()\n",
    "df['Dead stock'] = le.fit_transform(df['Dead stock'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 3** Preparing X and y arrays (Total Marks: 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Create a numpy array `y` from the first 80% observations of `Dead stock` column from `df` (2.5 marks)   \n",
    "- Create a numpy array `X`  from the first 80% observations of all the remaining variables in `df` (2.5 marks)   \n",
    "\n",
    "(Total: 5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "import numpy as np\n",
    "\n",
    "#  define cutoff index for first 80%\n",
    "split_index = int(len(df) * 0.8)\n",
    "\n",
    "# create target array y (Dead stock)\n",
    "y = df['Dead stock'].iloc[:split_index].to_numpy()\n",
    "\n",
    "# create feature array X (all other variables except Dead stock)\n",
    "X = df.drop(columns=['Dead stock']).iloc[:split_index].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2**. \n",
    "\n",
    "- Use an appropriate `sklearn` library we used in class to create `y_train`, `y_test`, `X_train` and `X_test` by splitting the data into 80% train and 20% test datasets (2.5 marks) \n",
    "    - Set random_state to 31 and stratify the subsamples so that train and test datasets have roughly equal proportions of the target's class labels \n",
    "- Standardise the data to mean zero and variance one using an approapriate `sklearn` library (2.5 marks)   \n",
    "\n",
    "\n",
    "(Total: 5 marks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- provide your code here -----\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the dataset into 80% train and 20% test with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.2, random_state = 31, stratify = y\n",
    ")\n",
    "\n",
    "# Standardizing the data (mean = 0, variance = 1)\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "\n",
    "X_train_scaled = sc.transform(X_train)\n",
    "X_test_scaled = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "**Problem 4**. Training Models and Interpretation (Total Marks: 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1**. \n",
    "\n",
    "- Train one linear classifier we studied in class using standardised data (6 marks)\n",
    "- Compute and print training and test dataset accuracies (4 marks)\n",
    "\n",
    "(Total: 10 marks)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9963541666666667\n",
      "Test Accuracy: 0.9958333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ---- provide your code here -----\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train logistic regression classifier\n",
    "lr = LogisticRegression(C=1, random_state=31, solver='lbfgs', multi_class='ovr')\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Compute and print accuracies\n",
    "print(\"Training Accuracy:\", lr.score(X_train_scaled, y_train))\n",
    "print(\"Test Accuracy:\", lr.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q2.**\n",
    "\n",
    "- Train one nonlinear classifier we studied in class on the same dataset (6 marks)\n",
    "- Compute and print training and test dataset accuracies (4 marks)\n",
    "\n",
    "\n",
    "(Total: 10 marks)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy : 1.0\n",
      "Test Accuracy : 0.9979166666666667\n"
     ]
    }
   ],
   "source": [
    "# ---- provide answer here -----\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Training a nonlinear classifier (Random Forest) on the same train set\n",
    "#    RF does not require standardisation; X_train (unscaled) is fine.\n",
    "rf = RandomForestClassifier(n_estimators=25, criterion='gini', random_state=31, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Computing and printing training and test accuracies\n",
    "print(\"Training Accuracy :\", rf.score(X_train, y_train))\n",
    "print(\"Test Accuracy :\", rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"width:25%;margin-left:0;\"> \n",
    "\n",
    "**Q3**. \n",
    "\n",
    "- Comment on the accuracy results obtained from the two classifiers (6 marks)\n",
    "- Based on our investigation into Dead stock predictions in this assignment, which model would you recommend and why? (4 marks)\n",
    "- What **feature(s)** do you believe has/have higest impact on predicting the outcome variable? Why?\n",
    "\n",
    "\n",
    "(Total: 10 marks)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- provide your text answer here ----\n",
    "Comment on the Accuracy: \n",
    "\n",
    "Logistic Regression achieved consistently high accuracy, with training around 99.6% and test accuracy 99.5%, and showed very little variation across C values from 0.5 to 10. This stability indicates that changes in regularization strength had minimal impact, reflecting the dataset’s near-linear separability.\n",
    "Random Forest also performed extremely well, reaching training accuracy of 100% and test accuracy between 99.79% and 100%. With a smaller number of trees (n_estimators=10), it already achieved 99.95% training and 99.79% test accuracy. Increasing the number of trees to 500 provided slightly more stable results but the combination of low estimators and entropy brought the best results. I believe the best result was achieved with n_estimators set to 10 and criterion set to entropy, where the training accuracy reached 99.94% and the test accuracy was 100%\n",
    "\n",
    "Recommended Model: \n",
    "Based on our investigation, I would recommend Random Forest for predicting Dead stock. While Logistic Regression performed very well, achieving around 99.6% training accuracy and 99.6% test accuracy, Random Forest achieved even higher scores with 100% training accuracy and about 99.8% test accuracy. This consistent outperformance, even by a small margin, shows that Random Forest was better able to capture the complex interactions among features such as stock age, turnover, and ABC categories.\n",
    "\n",
    "The best configuration was observed with n_estimators = 10 and criterion = entropy, where the model reached 99.94% training accuracy and a perfect 100% test accuracy. Increasing the number of trees (e.g., 500) improved stability further, though it introduced a mild risk of overfitting.\n",
    "\n",
    "Therefore, Random Forest is the recommended model. It not only delivers the strongest predictive performance but also highlights the most influential features, supporting both accurate forecasting and business decision-making.\n",
    "\n",
    "High Impact Features: \n",
    "The features with the greatest impact on predicting Dead stock are “Over 2 years Qty” , “ABC Class\",  “% of over 2 year”. These variables reflect both the age of stock and its ABC classification, which are key signals of whether items are likely to become unsellable. In particular, ABC Class is critical because the categories “Not Purchasing” and “End of Life” almost always correspond to Dead stock = 1, making them the clearest indicators of risk. Similarly, “Over 2 years Qty” and “% of over 2 year” measure how long inventory has been idle, and older stock is far less likely to sell. Supporting features such as Inventory Turn and Over 3 Years Quantity also reinforce the role of turnover speed and long-term stock aging. Overall, the combination of stock age and ABC Class—especially “Not Purchasing” and “End of Life”—emerges as the most influential driver of dead stock prediction, consistent with business logic that older and low-priority items are most at risk of becoming unsellable.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marking Criteria\n",
    "\n",
    "To achieve a perfect score, your solutions must adhere to the criteria outlined below:\n",
    "\n",
    "- Ensure that all numerical answers are accurate.\n",
    "- Utilize the exact Python functions and libraries specified within the assignment instructions.\n",
    "- For any written responses, provide accurate information, articulated in clear, complete sentences.\n",
    "- Do not add extra cells beyond what is provided in the notebook.\n",
    "- Do not print output with your code unless explicitly instructed to do so.\n",
    "- Maintain a clean and organised notebook layout that is easy to follow.\n",
    "- Marks will be deducted for not following the above instructions.\n",
    "    \n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
